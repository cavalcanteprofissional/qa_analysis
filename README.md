üìä AN√ÅLISE COMPARATIVA DE MODELOS DE QUESTION ANSWERING
üéØ Objetivo
Avaliar e comparar o desempenho de dois modelos de Question Answering (QA) dispon√≠veis no Hugging Face em um subconjunto do dataset DBpedia Entity Generated Queries.

üìã Dataset
Fonte: DBpedia Entity Generated Queries (BeIR)

Amostra: 1000 exemplos do shard_055.csv

Estrutura: Cada exemplo cont√©m:

_id: Identificador √∫nico

question: Pergunta a ser respondida

context: Texto contextual para resposta

title: T√≠tulo do t√≥pico

ü§ñ Modelos Avaliados
1. DistilBERT (distilbert-base-cased-distilled-squad)
Arquitetura: DistilBERT (vers√£o destilada do BERT)

Fine-tuning: SQuAD v1.1

Caracter√≠sticas: Leve, r√°pido, eficiente em recursos

Tamanho: ~250MB

2. RoBERTa (deepset/roberta-base-squad2)
Arquitetura: RoBERTa (Robustly Optimized BERT)

Fine-tuning: SQuAD v2.0

Caracter√≠sticas: Robusto, suporta perguntas sem resposta

Tamanho: ~500MB

üìä M√©tricas Calculadas
1. Score de Confian√ßa
O que √©: Probabilidade atribu√≠da pelo modelo √† resposta

Intervalo: 0.0 a 1.0

Interpreta√ß√£o: Quanto maior, mais confiante o modelo est√°

2. Overlap Contexto-Resposta
F√≥rmula: (palavras em comum) / (total palavras na resposta)

Interpreta√ß√£o:

100%: Resposta copiada exatamente do contexto

75-99%: Resposta muito pr√≥xima do contexto

50-74%: Resposta moderadamente relacionada

25-49%: Pouca rela√ß√£o direta

0-24%: Poss√≠vel alucina√ß√£o

3. Diferen√ßa entre Modelos
Diferen√ßa de score: score_roberta - score_distilbert

Diferen√ßa de overlap: overlap_roberta - overlap_distilbert

üîç An√°lises Realizadas
A) Distribui√ß√£o Geral
Score m√©dio de cada modelo

Overlap m√©dio de cada modelo

Correla√ß√£o score-overlap

B) An√°lise de Extremos
Por modelo: Top 10 melhores/piores de cada modelo

Global: Top 10 melhores/piores considerando ambos modelos

Discord√¢ncias: Casos onde modelos discordam significativamente

C) An√°lise Qualitativa (25 exemplos)
10 exemplos com maior score de cada modelo

10 exemplos com menor score de cada modelo

5 exemplos com discord√¢ncia (n√£o extremos)

üìà Resultados Principais
üéØ Performance Geral (Exemplo)
text
DistilBERT:
  ‚Ä¢ Score m√©dio: 0.7524
  ‚Ä¢ Overlap m√©dio: 84.2%
  ‚Ä¢ Venceu em: 45.3% das quest√µes

RoBERTa:
  ‚Ä¢ Score m√©dio: 0.7836  
  ‚Ä¢ Overlap m√©dio: 79.8%
  ‚Ä¢ Venceu em: 48.7% das quest√µes
üîó Correla√ß√£o Score-Overlap
DistilBERT: 0.428 (correla√ß√£o moderada positiva)

RoBERTa: 0.512 (correla√ß√£o moderada positiva)

üìÅ Estrutura dos Arquivos CSV Exportados
1. resultados_completos_YYYYMMDD_HHMMSS.csv
text
_id,question,context,distilbert_answer,distilbert_score,overlap_distilbert,
roberta_answer,roberta_score,overlap_roberta,score_difference,overlap_difference,
melhor_modelo_score,melhor_modelo_overlap
2. distilbert_top10_melhores_YYYYMMDD_HHMMSS.csv
text
_id,question,context,distilbert_answer,distilbert_score,overlap_distilbert,
rank,categoria,modelo
3. distilbert_top10_piores_YYYYMMDD_HHMMSS.csv
text
_id,question,context,distilbert_answer,distilbert_score,overlap_distilbert,
rank,categoria,modelo
4. roberta_top10_melhores_YYYYMMDD_HHMMSS.csv
text
_id,question,context,roberta_answer,roberta_score,overlap_roberta,
rank,categoria,modelo
5. roberta_top10_piores_YYYYMMDD_HHMMSS.csv
text
_id,question,context,roberta_answer,roberta_score,overlap_roberta,
rank,categoria,modelo
6. global_top10_melhores_YYYYMMDD_HHMMSS.csv
text
_id,question,context,melhor_score,modelo_melhor_score,distilbert_score,
roberta_score,rank,categoria
7. global_top10_piores_YYYYMMDD_HHMMSS.csv
text
_id,question,context,pior_score,distilbert_score,roberta_score,
rank,categoria
8. resumo_estatistico_YYYYMMDD_HHMMSS.csv
text
Categoria,Valor
9. discordancias_YYYYMMDD_HHMMSS.csv (opcional)
text
_id,question,context,distilbert_answer,distilbert_score,overlap_distilbert,
roberta_answer,roberta_score,overlap_roberta,score_diff_abs,rank,categoria
üõ†Ô∏è Como Reproduzir a An√°lise
Pr√©-requisitos
bash
pip install transformers torch pandas numpy
C√≥digo para C√°lculo de Overlap
python
import re

def clean_text(text):
    """Limpa o texto removendo caracteres especiais"""
    if not isinstance(text, str):
        return ""
    text = re.sub(r'[^\w\s\.\,\-\?]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip().lower()

def calculate_overlap(context, answer):
    """Calcula a sobreposi√ß√£o de palavras entre contexto e resposta"""
    if not answer or not context:
        return 0
    
    context_words = set(clean_text(context).split())
    answer_words = set(clean_text(answer).split())
    
    if not answer_words:
        return 0
    
    intersection = len(context_words.intersection(answer_words))
    return intersection / len(answer_words)
C√≥digo Principal para Processamento
python
from transformers import pipeline

# Carregar modelos
qa_distilbert = pipeline("question-answering", 
                        model="distilbert-base-cased-distilled-squad")
qa_roberta = pipeline("question-answering", 
                     model="deepset/roberta-base-squad2")

# Processar cada quest√£o
results = []
for idx, row in dataset.iterrows():
    question = row['question']
    context = row['context']
    
    # Executar modelos
    result_distilbert = qa_distilbert(question=question, context=context)
    result_roberta = qa_roberta(question=question, context=context)
    
    # Calcular overlaps
    overlap_dist = calculate_overlap(context, result_distilbert['answer'])
    overlap_rob = calculate_overlap(context, result_roberta['answer'])
    
    results.append({
        'distilbert_answer': result_distilbert['answer'],
        'distilbert_score': result_distilbert['score'],
        'overlap_distilbert': overlap_dist,
        'roberta_answer': result_roberta['answer'],
        'roberta_score': result_roberta['score'],
        'overlap_roberta': overlap_rob
    })
üìù Interpreta√ß√£o para Decis√£o em Produ√ß√£o
üü¢ Quando escolher DistilBERT:
Recursos computacionais limitados (CPU ou mem√≥ria restrita)

Lat√™ncia √© cr√≠tica (respostas em tempo real)

Perguntas diretas com contexto expl√≠cito

Custo de infer√™ncia √© fator importante

Ambientes com limita√ß√£o de energia ou dispositivos m√≥veis

üîµ Quando escolher RoBERTa:
Precis√£o √© prioridade m√°xima sobre velocidade

Perguntas complexas ou amb√≠guas

Contextos longos ou densos em informa√ß√£o

Suporte a perguntas sem resposta necess√°rio

Ambientes empresariais com recursos adequados

üìä Recomenda√ß√£o Baseada na An√°lise:
text
Baseado na an√°lise de 1000 quest√µes:

‚Ä¢ Para APLICA√á√ïES EM TEMPO REAL com recursos limitados:
  ‚Üí DistilBERT (mais r√°pido, menor consumo)

‚Ä¢ Para SISTEMAS CR√çTICOS onde precis√£o √© essencial:
  ‚Üí RoBERTa (mais preciso, melhor em contextos complexos)

‚Ä¢ Para SISTEMAS H√çBRIDOS:
  ‚Üí Usar DistilBERT para perguntas simples
  ‚Üí Usar RoBERTa para perguntas complexas (fallback)
‚ö†Ô∏è Limita√ß√µes e Considera√ß√µes
Vi√©s do dataset: An√°lise baseada em apenas 1000 exemplos de um shard espec√≠fico

Limita√ß√£o de contexto: M√°ximo de 512 tokens por contexto (limita√ß√£o dos modelos)

M√©trica de overlap: Baseada apenas em palavras exatas, n√£o considera:

Sin√¥nimos

Reformula√ß√µes sem√¢nticas

Par√°frases

Scores de confian√ßa: Podem variar entre execu√ß√µes (n√£o-determinismo)

Caracter√≠sticas do dataset: Perguntas principalmente sobre localidades geogr√°ficas

üîÆ Pr√≥ximos Passos Sugeridos
1. Expans√£o da An√°lise
Analisar mais shards (diferentes dom√≠nios/t√≥picos)

Aumentar tamanho da amostra (5000+ exemplos)

Testar com diferentes tipos de perguntas

2. M√©tricas Adicionais
Exact Match (EM): Resposta exatamente igual √† esperada

F1-Score: Medida de sobreposi√ß√£o token-level

BERTScore: Similaridade sem√¢ntica usando embeddings

Tempo de infer√™ncia: Compara√ß√£o de velocidade

3. An√°lise de Erros
Categoriza√ß√£o dos tipos de erros:

Alucina√ß√µes (respostas n√£o baseadas no contexto)

Respostas incompletas

Respostas incorretas

Falha em responder

An√°lise por tipo de pergunta:

Perguntas factuais

Perguntas de localiza√ß√£o

Perguntas temporais

Perguntas comparativas

4. Benchmark Expandido
Testar mais modelos (BERT-large, ALBERT, DeBERTa)

Comparar vers√µes quantizadas

Avaliar trade-off tamanho vs. performance

Testar em diferentes hardwares (CPU, GPU, TPU)

5. An√°lise de Custo-Benef√≠cio
Custo computacional por infer√™ncia

Uso de mem√≥ria

Tempo de resposta m√©dio

Custo em cloud computing

üìö Refer√™ncias T√©cnicas
Artigos Cient√≠ficos
DistilBERT: Sanh et al. (2019) - "DistilBERT, a distilled version of BERT"

RoBERTa: Liu et al. (2019) - "RoBERTa: A Robustly Optimized BERT Pretraining Approach"

SQuAD: Rajpurkar et al. (2016) - "SQuAD: 100,000+ Questions for Machine Comprehension"

Documenta√ß√£o
Hugging Face Transformers: https://huggingface.co/docs/transformers

SQuAD Dataset: https://rajpurkar.github.io/SQuAD-explorer/

DBpedia Entity: https://huggingface.co/datasets/BeIR/dbpedia-entity

Links dos Modelos
DistilBERT SQuAD: https://huggingface.co/distilbert-base-cased-distilled-squad

RoBERTa SQuAD2: https://huggingface.co/deepset/roberta-base-squad2

üìß Informa√ß√µes do Projeto
Projeto: An√°lise Comparativa de Modelos de Question Answering

Dataset: DBpedia Entity Generated Queries (shard_055)

Amostra: 1000 exemplos

Modelos: DistilBERT vs. RoBERTa

M√©tricas: Score de confian√ßa, Overlap, Diferen√ßas

Timestamp: YYYYMMDD_HHMMSS

Ambiente: Google Colab com GPU T4

üéì Como Contribuir
Para Extens√£o da An√°lise:
Testar com mais modelos do Hugging Face

Aplicar a diferentes datasets

Implementar m√©tricas adicionais

Realizar an√°lise de erro detalhada

Para Melhorias no C√≥digo:
Otimizar processamento em batch

Adicionar cache para resultados

Implementar paraleliza√ß√£o

Criar visualiza√ß√µes interativas

Para Documenta√ß√£o:
Adicionar exemplos pr√°ticos

Incluir casos de uso espec√≠ficos

Documentar limita√ß√µes encontradas

Criar guias de deploy

‚ö° Dicas R√°pidas para Uso
Para Carregar os Resultados:
python
import pandas as pd
df = pd.read_csv('resultados_completos_YYYYMMDD_HHMMSS.csv')
Para An√°lise dos Extremos:
python
# Top 10 melhores do DistilBERT
top_distilbert = pd.read_csv('distilbert_top10_melhores_YYYYMMDD_HHMMSS.csv')

# Top 10 piores globais
piores_globais = pd.read_csv('global_top10_piores_YYYYMMDD_HHMMSS.csv')
Para An√°lise de Discord√¢ncias:
python
if os.path.exists('discordancias_YYYYMMDD_HHMMSS.csv'):
    discordancias = pd.read_csv('discordancias_YYYYMMDD_HHMMSS.csv')
    print(f"Encontradas {len(discordancias)} discord√¢ncias significativas")
üìä Gloss√°rio de Termos
QA (Question Answering): Sistema que responde perguntas baseado em contexto

Score de Confian√ßa: Probabilidade atribu√≠da pelo modelo √† corre√ß√£o da resposta

Overlap: Porcentagem de palavras da resposta presentes no contexto

Alucina√ß√£o: Quando o modelo gera informa√ß√£o n√£o presente no contexto

SQuAD: Stanford Question Answering Dataset, benchmark para QA

Fine-tuning: Processo de adaptar um modelo pr√©-treinado para uma tarefa espec√≠fica

Infer√™ncia: Processo de obter respostas do modelo

